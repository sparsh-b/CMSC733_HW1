{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sparsh-b/CMSC733_HW1/blob/main/CMSC733_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 1: \n",
        "\n",
        "Name: \n",
        "\n",
        "UID:\n",
        "\n",
        "Please submit to Gradescope\n",
        "- a PDF containing all outputs (by executing **Run all**)\n",
        "- your ipynb notebook containing all the code\n",
        "\n",
        "I understand the policy on academic integraty (collaboration and the use of online material).\n",
        "Please sign your name here: \n"
      ],
      "metadata": {
        "id": "Jf-_uA8P5Hf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part A: Hybrid Image (25 Points)"
      ],
      "metadata": {
        "id": "9ftoiwAo45n5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "A hybrid image is the sum of a *low-pass filtered* version of the one image and a *high-pass filtered* version of a second image. There is a free parameter, which can be tuned for each image pair, which controls how much high frequency to remove from the first image and how much low frequency to leave in the second image. This is called the “cutoff-frequency”. In the paper it is suggested to use two cutoff frequencies (one tuned for each image) and you are free to try that, as well. In the starter code, the cutoff frequency is controlled by changing the standard deviation of the Gausian filter used in constructing the hybrid images. [This](https://drive.google.com/uc?id=187FjBJLwnYXhylx08Vdh1SAA3AO-imYv) is the sample example.\n",
        "\n",
        "NOTE: \n",
        "\n",
        "1. Reading [this](https://stanford.edu/class/ee367/reading/OlivaTorralb_Hybrid_Siggraph06.pdf) will help in understanding Part A.\n",
        "\n",
        "2. You can use any image processing libraries of your choice such as skimage or cv2; in python.\n",
        "\n",
        "We provided 7 pairs of aligned images. The alignment is important because it affects the perceptual grouping (read the paper for details). We encourage you to create additional examples (e.g. change of expression, morph between different objects, change over time, etc.).\n",
        "\n",
        "You are required to provide **THREE hybrid image results** and for ONE of your favorite result, please provide answers to the following **FOUR sub-parts** mentioned in the write-up."
      ],
      "metadata": {
        "id": "tJikaY9ICDYH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "**WARNING: Colab deletes all files everytime runtime is disconnected. Make sure to re-download the inputs when it happens.**"
      ],
      "metadata": {
        "id": "WR0sjl-f6qkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Data -- run this cell only one time per runtime\n",
        "!gdown 1KTDxPAkQam29YKtoX5dKPnLKpUOWCanC\n",
        "!unzip \"/content/hybrid_pyramid_input.zip\" -d \"/content/\""
      ],
      "metadata": {
        "id": "0uWzKA6i68ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code"
      ],
      "metadata": {
        "id": "g1DBek-_Btwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper Functions\n",
        "\n",
        "def vis_hybrid_image(hybrid_image):\n",
        "  scales = 5\n",
        "  scale_factor = 0.5\n",
        "  padding = 5\n",
        "  original_height = hybrid_image.shape[0]\n",
        "  num_colors = hybrid_image.shape[2] # counting how many color channels the input has\n",
        "  output = hybrid_image\n",
        "  cur_image = hybrid_image\n",
        "\n",
        "  for i in range(2, scales):\n",
        "      # add padding\n",
        "      output = np.concatenate((output, np.ones((original_height, padding, num_colors), dtype=int)), axis=1)      \n",
        "      # dowsample image;\n",
        "      width = int(cur_image.shape[1] * scale_factor)\n",
        "      height = int(cur_image.shape[0] * scale_factor)\n",
        "      dim = (width, height)\n",
        "      cur_image = cv2.resize(cur_image, dim, interpolation = cv2.INTER_LINEAR)\n",
        "      # pad the top and append to the output\n",
        "      tmp = np.concatenate((np.ones((original_height-cur_image.shape[0], cur_image.shape[1], num_colors)), cur_image), axis=0)\n",
        "      output = np.concatenate((output, tmp), axis=1)\n",
        "  \n",
        "  output = (output * 255).astype(np.uint8)\n",
        "  return output\n",
        "\n",
        "def read_image(image_path):\n",
        "  # YOUR CODE HERE\n",
        "  return image\n",
        "\n",
        "def gaussian_2D_filter((size,size), cutoff_frequency):\n",
        "  # YOUR CODE HERE\n",
        "  return filter\n",
        "\n",
        "def imgfilter(image, filter):\n",
        "  # YOUR CODE HERE\n",
        "  return \n",
        "\n",
        "def log_mag_FFT(image):\n",
        "  # YOUR CODE HERE\n",
        "  return output"
      ],
      "metadata": {
        "id": "NeXJU9eHmzMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary packages here\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "image_path1 = ?\n",
        "image_path2 = ?\n",
        "\n",
        "image_1 = read_image(image_path1)\n",
        "image_2 = read_image(image_path2)\n",
        "\n",
        "# YOUR CODE HERE: TUNE cutoff_frequency\n",
        "cutoff_frequency = ?\n",
        "filter_size = cutoff_frequency*4+1\n",
        "\"\"\"cutoff_frequency is the standard deviation, in pixels, of the \n",
        "Gaussian blur that will remove the high frequencies from one image (image_1) and \n",
        "remove the low frequencies from another image (image_2) (to do so, subtract a blurred\n",
        "version from the original version). You will want to tune this for every image pair to get the best results.\"\"\"\n",
        "\n",
        "filter = gaussian_2D_filter((filter_size, filter_size), cutoff_frequency)\n",
        "plt.imshow(filter)\n",
        "\n",
        "\"\"\"Use imgfilter() to create 'low_frequencies' and 'high_frequencies' and then combine them to create 'hybrid_image'.\n",
        "Remove the high frequencies from image_1 by blurring it. The amount of blur that works best will vary with different image pairs.\"\"\"\n",
        "\n",
        "blurred_image1 = imgfilter(image_1, filter=filter)\n",
        "# YOUR CODE HERE.\n",
        "low_frequencies = ?\n",
        "\n",
        "\"\"\"Remove the low frequencies from image_2. The easiest way to do this is to\n",
        "subtract a blurred version of image_2 from the original version of image_2.\n",
        "This will give you an image centered at zero with negative values.\"\"\"\n",
        "\n",
        "# YOUR CODE HERE\n",
        "high_frequencies = ?\n",
        "\n",
        "\"\"\"Combine the high frequencies and low frequencies to obtain hybrid_image.\"\"\"\n",
        "# YOUR CODE HERE\n",
        "hybrid_image= ?\n",
        "\n",
        "\"\"\"Firstly, visualize low_frequencies, high_frequencies, and the hybrid image.\"\"\"\n",
        "# YOUR CODE HERE.\n",
        "\n",
        "\"\"\"Secondly, also visualize log magnitude of Fourier Transform of the above.\n",
        "HINT: You may use np.log(np.abs(np.fft.fftshift(np.fft.fft2(image)))) to achieve it.\"\"\"\n",
        "FFT_image = log_mag_FFT(image1)\n",
        "# YOUR CODE HERE.\n",
        "\n",
        "\"\"\"Thirdly, visualize hybrid_image_scale using helper function vis_hybrid_image.\n",
        "Lastly, save all your outputs.\"\"\"\n",
        "# YOUR CODE HERE.\n"
      ],
      "metadata": {
        "id": "9F0nOPhWn95d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Write-up**\n",
        "\n",
        "\n",
        "1.   Provide the original and filtered images.\n",
        "2.   Provide the the hybrid image and hybrid_image_scale using given helper function *vis_hybrid_image*.\n",
        "3.   Log magnitude of the Fourier transform of the two original images, the filtered images, and the hybrid image.\n",
        "4.   Briefly explain how this works, using your favorite results as illustrations."
      ],
      "metadata": {
        "id": "zmN8MtK2mOUz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Include your write-up here**"
      ],
      "metadata": {
        "id": "I06DhqYI4YI7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part B: Pyramid Image (25 Points)"
      ],
      "metadata": {
        "id": "UurFy8hB5BeP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "Choose an image that has interesting variety of textures (from Flickr or your own images). The images should be atleast 640X480 pixels and converted to grayscale. Write code for a Gaussian and Laplacian pyramid of level N (use for loops). In each level, the resolution should be reduced by a factor of 2. Show the pyramids for your chosen image in your write-up. Here is an [example](https://drive.google.com/uc?id=17Y287EA-GJ2z0wtm_M7StIWsXyFeHvrz)."
      ],
      "metadata": {
        "id": "baagTPK5aa9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "**WARNING: Colab deletes all files everytime runtime is disconnected. Make sure to re-download the inputs when it happens.**"
      ],
      "metadata": {
        "id": "qWCfxrUA7LNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Data -- run this cell only one time per runtime\n",
        "!gdown 1KTDxPAkQam29YKtoX5dKPnLKpUOWCanC\n",
        "!unzip \"/content/hybrid_pyramid_input.zip\" -d \"/content/\""
      ],
      "metadata": {
        "id": "J5aF6Biq7LvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code"
      ],
      "metadata": {
        "id": "cUgcCzJRAQJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Populate Helper Functions:\n",
        "\n",
        "def pyramidsGL(image, num_levels):\n",
        "  ''' Creates Gaussian (G) and Laplacian (L) pyramids of level \"num_levels\" from image im. \n",
        "  G and L are list where G[i], L[i] stores the i-th level of Gaussian and Laplacian pyramid, respectively. '''\n",
        "  # YOUR CODE HERE\n",
        "  return G, L\n",
        "\n",
        "def displayPyramids(G, L):\n",
        "  '''Role of this function is to display intensity and Fast Fourier Transform (FFT) images of pyramids.\n",
        "  NOTE: You may re-use your helper function  \"log_mag_FFT\" to compute this.'''\n",
        "  # YOUR CODE HERE\n",
        "  return\n",
        "\n",
        "def reconstructLaplacianPyramid(L):\n",
        "  '''Given a Laplacian Pyramid L, reconstruct an image img.'''\n",
        "  # YOUR CODE HERE \n",
        "  return img\n",
        "  "
      ],
      "metadata": {
        "id": "ywdGq5E55thm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" \n",
        "Steps:\n",
        "1. Load an image im.\n",
        "2. Call function pyramidsGL with image and num_levels = 5\n",
        "3. Call function displayPyramids with G, L\n",
        "4. Call function reconstructLaplacianPyramid with the generated L\n",
        "5. Compute reconstruction error with L2 norm and print the error value.\n",
        "\"\"\"\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "7e60BvZg5Nsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Write-up**\n",
        "\n",
        "1. (10 Points) Display a Gaussian and Laplacian pyramid of level 5 (using your code). It should be formatted similar to [this](https://drive.google.com/file/d/1mAommQeJsp7WS8QCrZRcr8cQiltPPOh2/view?usp=sharing) figure.\n",
        "\n",
        "2. (10 Points) Display the FFT amplitudes of your Gaussian/Laplacian pyramids Appropriate display ranges (from 0 to 1) should be chosen so that the changes in frequency in different levels of the pyramid are clearly visible. Explain  what the Laplacian and Gaussian pyramids are doing in terms of frequency. [This](https://drive.google.com/file/d/1BqTPKq6Mqqxl5jNNPkvx4JOA5MRgVq08/view?usp=sharing) looks like the expected output.\n",
        "\n",
        "3. (5 Points) Image Reconstruction\n"
      ],
      "metadata": {
        "id": "8AmYt5Vu9BGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Include your write-up here**"
      ],
      "metadata": {
        "id": "lRUDQ9Xk5e3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part C: Edge detection (25 points)"
      ],
      "metadata": {
        "id": "F6UrONbD4LcH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "The main steps of edge detection are: (1) assign a score to each pixel; (2) find local maxima along the direction perpendicular to the edge. Sometimes a third step is performed where local evidence is propagated so that long contours are more confident or strong edges boost the confidence of nearby weak edges. Optionally, a thresholding step can then convert from soft boundaries to hard binary boundaries. Here are sample outputs.\n",
        "\n",
        "<table><tr>\n",
        "<td> <img src=\"https://drive.google.com/uc?id=1orUji5-1CzjWmHk0g5y5kOVFhshNfhN8\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
        "<td> <img src=\"https://drive.google.com/uc?id=1npyMjhlRAeP1GaukV38SOlCe-O0whX37\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
        "</tr></table>\n",
        "<table><tr>\n",
        "<td> <img src=\"https://drive.google.com/uc?id=1TX54zNTG6q5ajitwV024FS-nOJiVP2VN\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
        "<td> <img src=\"https://drive.google.com/uc?id=1dPPSFDmakh8DQwlpTYmNXJPNHapdZF8S\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
        "</tr></table>\n",
        "\n",
        "**Hint:** Reading these papers will help understanding and may help with the programming assignment.\n",
        "\n",
        "[The design and use of steerable filters](http://people.csail.mit.edu/billf/papers/steerpaper91FreemanAdelson.pdf)\n",
        "\n",
        "[Berkeley Pb Detector](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/papers/mfm-pami-boundary.pdf)\n",
        "\n",
        "[Multi-scale edge detection](https://home.ttic.edu/~xren/publication/xren_eccv08_multipb.pdf)"
      ],
      "metadata": {
        "id": "EGoM9Ra_4QLS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "We have provided 50 test images and the codes associated to download the unzip the data. Your job is to build a simple gradient-based edge detector and to extend it using multiple oriented filters.\n",
        "\n",
        "**WARNING: Colab deletes all files everytime runtime is disconnected. Make sure to re-download the inputs when it happens.**"
      ],
      "metadata": {
        "id": "LyJ9rIej5T_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Data -- run this cell only one time per runtime\n",
        "!gdown 1zgblBWTQ847yZKnRmM1QrRiEWu1WvEo7\n",
        "!unzip \"/content/edge_detection_inputs.zip\" -d \"/content/\""
      ],
      "metadata": {
        "id": "Jcjx6ntsvF7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary packages\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow ## Use this to show image in colab\n",
        "\n",
        "img_path = \"/content/edge_detection_inputs/????.jpg\" ## add the path here\n",
        "img = cv2.imread(img_path)"
      ],
      "metadata": {
        "id": "H3XyjNp04UHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subpart 1: Simple edge detection (8 points)\n",
        "Build a simple gradient-based edge detector that includes the following functions\n",
        "```\n",
        "def gradientMagnitude(im, signma)\n",
        "```\n",
        "This function should take an RGB image as input, smooth the image with Gaussian std=sigma, compute the x and y gradient values of the smoothed image, and output image maps of the gradient magnitude and orientation at each pixel. You can compute the gradient magnitude of an RGB image by taking the L2-norm of the R, G, and B gradients. The orientation can be computed from the channel corresponding to the largest gradient magnitude. The overall gradient magnitude is the L2-norm of the x and y gradients. mag and theta should be the same size as im.\n",
        "\n",
        "```\n",
        "def edgeGradient(im):\n",
        "```\n",
        "This function should use gradientMagnitude to compute a soft boundary map and then perform non-maxima suppression. For this assignment, it is acceptable to perform non-maxima suppression by retaining only the magnitudes along the binary edges produce by the Canny edge detector: `cv2.Canny(im)`. \n",
        "\n",
        "If desired, the boundary scores can be rescaled, e.g., by raising to an exponent: `mag2 = mag.^0.7` , which is primarily useful for visualization. \n"
      ],
      "metadata": {
        "id": "ddS6DvhA4RNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradientMagnitude(im, sigma):\n",
        "  '''\n",
        "  im: input image\n",
        "  sigma: standard deviation value to smooth the image\n",
        "\n",
        "  outputs: gradient magnitude and gradient direction of the image\n",
        "  '''\n",
        "  ## YOUR CODE HERE\n",
        "  pass\n",
        "\n",
        "def edgeGradient(im):\n",
        "  '''\n",
        "  im: input image\n",
        "\n",
        "  output: a soft boundary map of the image\n",
        "  '''\n",
        "  ## YOUR CODE HERE\n",
        "  pass"
      ],
      "metadata": {
        "id": "6mO9BeLc4WWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subpart 2: Improved Edge Detection (8 points)\n",
        "Try to improve your results using a set of oriented filters, rather than the simple derivative of Gaussian approach above, including the following functions:\n",
        "```\n",
        "def orientedFilterMagnitude(im):\n",
        "```\n",
        "Computes the boundary magnitude and orientation using a set of oriented filters, such as elongated Gaussian derivative filters. Explain your choice of filters in the write-up. Use at least four orientations. One way to combine filter responses is to compute a boundary score for each filter (simply by filtering with it) and then use the max and argmax over filter responses to compute the magnitude and orientation for each pixel.\n",
        "```\n",
        "def edgeOrientedFilters(im):\n",
        "```\n",
        "Similar to Subpart 1, this should call orientedFilterMagnitude, perform the non-maxima suppression, and output the final soft edge map."
      ],
      "metadata": {
        "id": "VhF3zcUT4Yen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def orientedFilterMagnitude(im):\n",
        "  '''\n",
        "  im: input image\n",
        "\n",
        "  outputs: gradient magnitude and gradient direction of the image\n",
        "  '''\n",
        "  ## YOUR CODE HERE\n",
        "  pass\n",
        "\n",
        "def edgeOrientedFilters(im):\n",
        "  '''\n",
        "  im: input image\n",
        "\n",
        "  output: a soft boundary map of the image\n",
        "  '''\n",
        "  ## YOUR CODE HERE\n",
        "  pass"
      ],
      "metadata": {
        "id": "2Fsu68Bn4hoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write-up (9 points)\n",
        "\n",
        "1.   **(2 points)** Description of any design choices and parameters\n",
        "2.   **(1 points)** The bank of filters used for Subpart 2 ([plt.imshow with extent](https://stackoverflow.com/questions/13384653/imshow-extent-and-aspect/13390798#13390798) or [cv2.normalize to convert output into grayscale](https://stackoverflow.com/questions/39808545/implement-mat2gray-in-opencv-with-python) may help with visualization)\n",
        "3.   **(5 points)** Qualitative results: choose five example images; show input images and outputs of each edge detector\n",
        "4.   **(1 points)** Discuss the quality of your outputs and state one possible way for improvement. Improvements could provide, for example, a better boundary pixel score or a better suppression technique. Your idea could come from a paper you read, but cite any sources of ideas.\n",
        "\n"
      ],
      "metadata": {
        "id": "Dj1FsFQg5u9n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Include your write-up here**"
      ],
      "metadata": {
        "id": "fEa_-lFkux7C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part D: Template Matching (25 points) "
      ],
      "metadata": {
        "id": "J4vgQB7R7YdG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "The goal of this part is to build a template maching algorithm for where's waldo puzzle. \n",
        "The end product should be finding waldo in puzzle images. \n"
      ],
      "metadata": {
        "id": "h27omwbf4pAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "**WARNING: Colab deletes all files everytime runtime is disconnected. Make sure to re-download the inputs when it happens.**\n"
      ],
      "metadata": {
        "id": "ZOiWfqu74wet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Data -- run this cell only one time per runtime\n",
        "!gdown 1_PHimFhPSajbTWzAL6-PwM803uzA7Ymb\n",
        "!unzip \"/content/Part4_data.zip\" -d \"/content/\""
      ],
      "metadata": {
        "id": "yBSS8GTN77B1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0359ac78-ed78-4b83-b7fd-36d472634203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_PHimFhPSajbTWzAL6-PwM803uzA7Ymb\n",
            "To: /content/Part4_data.zip\n",
            "\r  0% 0.00/2.04M [00:00<?, ?B/s]\r100% 2.04M/2.04M [00:00<00:00, 184MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code\n",
        "\n",
        "We provide the following functions for plotting your results \n"
      ],
      "metadata": {
        "id": "3o5Nh1Tc7zZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_image(im,title,xticks=[],yticks= [],cv2 = True):\n",
        "    \"\"\"\n",
        "    im :Image to plot\n",
        "    title : Title of image \n",
        "    xticks : List of tick values. Defaults to nothing\n",
        "    yticks :List of tick values. Defaults to nothing \n",
        "    cv2 :Is the image cv2 image? cv2 images are BGR instead of RGB. Default True\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.imshow(im[:,:,::-1])\n",
        "    plt.title(title)\n",
        "    plt.xticks(xticks)\n",
        "    plt.yticks(yticks)"
      ],
      "metadata": {
        "id": "L7GrNdvQ4yM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is always advised for you to visualize and get familar with waldo and puzzle image. \n",
        "\n",
        "Visualize both the waldo and puzzle images."
      ],
      "metadata": {
        "id": "UqdglV474z9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR CODE HERE"
      ],
      "metadata": {
        "id": "faADukkZ42Vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Subpart 1: Template Matching with OpenCV\n",
        "OpenCV provide Template Matching functions below link. \n",
        "\n",
        "https://docs.opencv.org/4.x/d4/dc6/tutorial_py_template_matching.html\n",
        "\n",
        "You can use this function for implementing simple where's waldo algorithm.\n",
        "\n",
        "This part helps you to understand the concepts of Template Matching and OpenCV Library. This is just for reference."
      ],
      "metadata": {
        "id": "IYBFgc6O4rlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(waldoHeight, waldoWidth) = waldo.shape[:2]\n",
        "\n",
        "result = cv2.matchTemplate(map, waldo, cv2.TM_CCOEFF)\n",
        "(_, _, minLoc, maxLoc) = cv2.minMaxLoc(result)\n",
        "\n",
        "# grab the bounding box of waldo and extract him from the puzzle image\n",
        "topLeft = maxLoc\n",
        "botRight = (topLeft[0] + waldoWidth, topLeft[1] + waldoHeight)\n",
        "roi = map[topLeft[1] : botRight[1], topLeft[0] : botRight[0]]\n",
        "\n",
        "# construct a darkened transparent 'layer' to darken everything\n",
        "# in the map except for Waldo\n",
        "mask = np.zeros(map.shape, dtype = \"uint8\")\n",
        "map = cv2.addWeighted(map, 0.25, mask, 0.75, 0)\n",
        "\n",
        "map[topLeft[1] : botRight[1], topLeft[0] : botRight[0]] = roi\n",
        "\n",
        "# display the images\n",
        "result_rgb = cv2.cvtColor(map, cv2.COLOR_RGB2BGR)\n",
        "plt.figure(figsize = (15, 15))\n",
        "plt.imshow(result_rgb)"
      ],
      "metadata": {
        "id": "CdjuPWm3464i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Subpart 2: Template Matching from Scratch\n",
        "\n",
        "Implement the Sum of Squared Distance (SSD) template matching algorithm from scratch (Don't use cv2.matchTemplate).\n",
        "\n",
        "Then, show the results of where's waldo for two puzzle images.\n",
        "\n",
        "Hints: You can borrow the codes from Part1 and Part2. Please read methods for matching with filters in Lecture Slide.  "
      ],
      "metadata": {
        "id": "yOtg3X-U47Y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR CODE HERE"
      ],
      "metadata": {
        "id": "XQhzo50n49s8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}